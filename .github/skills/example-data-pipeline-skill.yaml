name: data-pipeline
description: Expert in ETL and data processing workflows

instructions: |
  When planning a data pipeline project, structure work into these phases:
  
  1. **Data Extraction**: Source connections and data retrieval
     - Set up connectors to data sources (databases, APIs, files)
     - Implement scheduled or event-driven extraction
     - Handle authentication and connection pooling
     - Add retry logic and error handling
  
  2. **Data Transformation**: Processing and enrichment
     - Design transformation logic (cleaning, validation, enrichment)
     - Implement data quality checks
     - Create reusable transformation functions
     - Add data lineage tracking
  
  3. **Data Loading**: Destination setup and loading
     - Configure target database/warehouse schema
     - Implement batch or streaming loading
     - Add idempotency and deduplication
     - Set up incremental loading strategies
  
  4. **Monitoring & Observability**: Health and performance tracking
     - Configure logging for each pipeline stage
     - Set up alerts for failures and data quality issues
     - Create dashboards for pipeline metrics
     - Implement data reconciliation checks
  
  Consider idempotency, error handling, and data quality throughout.
  Maximize parallelism where independent transformations can run concurrently.

examples:
  - input: "Build ETL pipeline from REST API to data warehouse"
    tasks:
      - Create API connector with authentication
      - Implement data extraction scheduler
      - Build transformation functions for data cleaning
      - Design target warehouse schema
      - Implement batch loading with deduplication
      - Add error handling and retry logic
      - Configure monitoring and alerting
      - Create data quality validation tests
  
  - input: "Real-time streaming pipeline from Kafka to analytics DB"
    tasks:
      - Setup Kafka consumer with proper offset management
      - Implement real-time transformation logic
      - Configure target database for streaming writes
      - Add exactly-once processing guarantees
      - Setup monitoring dashboard
      - Create backfill process for historical data
